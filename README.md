# ğŸŒ©ï¸ Multimodal Cloud RAG Chatbot

**Ask anything about AWS, Azure, or GCP using Text â€¢ Voice â€¢ Image.**
---

## âœ¨ Highlights

* **Super Fast Inferencing** Bescause of GROQ
* **RAG over your cloud knowledge** using FAISS + Sentenceâ€‘Transformers
* **GROQ Llamaâ€‘3.3â€‘70B** for highâ€‘quality answers (via REST)
* **Modes:** Text chat Â· Voice input (Whisper) Â· Image caption â†’ query (BLIP)
* **Streamlit UI** with a simple chat history

---
## Test My Work

https://multimodal-cloud-chatbot-d7u49vfmkjyow5t3ufjtcg.streamlit.app/

---

## ğŸ—ï¸ Architecture (at a glance)

```
[User] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Streamlit UI
   â”‚                          â”‚
   â”‚  text / voice / image    â”‚
   â–¼                          â–¼
Voice (Whisper)         Image (BLIP caption)
   â”‚                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Query text â—„â”€â”€â”˜
                           â”‚
                     RAG Pipeline
                (FAISS + embeddings)
                           â”‚
                     GROQ LLM (API)
                           â”‚
                           â–¼
                      Final Answer
```

---

## ğŸ“ Project Structure (expected)

```
multimodal-cloud-chatbot/
â”œâ”€ backend/
â”‚  â”œâ”€ rag_pipeline.py           # FAISS retriever + GROQ client
â”‚  â””â”€ vectorstore/
â”‚     â”œâ”€ index.faiss
â”‚     â””â”€ index.pkl              # place your built index here
â”‚  â””â”€ multimodal/
â”‚     â””â”€ caption_blip.py        # BLIP image captioning
â”œâ”€ voice/
â”‚  â”œâ”€ speech_to_text.py         # faster-whisper STT
â”‚  â””â”€ text_to_speech.py         # pyttsx3 TTS
â”œâ”€ streamlit_app.py             # UI (Text/Voice/Image)
â”œâ”€ requirements.txt             # runtime deps
â””â”€ .env                         # GROQ_API_KEY=...
```

> If your repo uses a flat layout, either move files into these folders or adjust imports in `streamlit_app.py` accordingly.

---

## âš™ï¸ Prerequisites

* **Python 3.10+** (3.11 OK)
* **FFmpeg** (required for audio handling; e.g., `choco install ffmpeg` on Windows, `sudo apt-get install ffmpeg` on Ubuntu)
* **Git LFS (optional)** if you store large FAISS indexes in Git

---

## ğŸš€ Quickstart

1. **Clone & venv**

```bash
git clone https://github.com/kunal-arora-1411/multimodal-cloud-chatbot.git
cd multimodal-cloud-chatbot
python -m venv .venv && source .venv/bin/activate  # on Windows: .venv\Scripts\activate
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
# plus a few needed by the code
pip install requests faster-whisper sentence-transformers langchain-core langchain-community
```

> If you hit platform issues with `faster-whisper`, try `pip install --upgrade ctranslate2`.

3. **Environment variables**
   Create a `.env` file at the repo root:

```ini
GROQ_API_KEY=your_groq_key_here
```

4. **Prepare the vector store**
   Place your FAISS files under `backend/vectorstore/` as:

```
backend/vectorstore/index.faiss
backend/vectorstore/index.pkl
```

(See **Build/Update the Knowledge Base** below if you need to generate these files.)

5. **Run the app**

```bash
streamlit run streamlit_app.py
```

Open the URL shown by Streamlit.

---

## ğŸ–¥ï¸ Using the App

### 1) Text mode

* Choose **Text** in the sidebar â†’ type your question.
* The app retrieves top-k chunks from FAISS and asks GROQ Llamaâ€‘3.3â€‘70B to answer with context.

### 2) Voice mode

* Choose **Voice** â†’ upload MP3/WAV.
* Audio is transcribed with **fasterâ€‘whisper** and used as the query.
* The answer can be played back using **pyttsx3**.

### 3) Image mode

* Choose **Image** â†’ upload PNG/JPG.
* A BLIP caption is generated and used as the query text.

---

## ğŸ” Secrets & Config

* **GROQ\_API\_KEY** must be present in your `.env`.
* (Optional) **TRANSFORMERS\_CACHE** can be set for BLIP model caching. If youâ€™re not on Windows, remove or change the path in `caption_blip.py`.

---

## ğŸ§© Key Components

* **RAG pipeline**: FAISS retriever + prompt builder + GROQ REST call
* **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`
* **LLM**: `llama-3.3-70b-versatile` via `https://api.groq.com/openai/v1/chat/completions`
* **Image captioning**: `Salesforce/blip-image-captioning-base`
* **STT**: `faster-whisper` (Whisper â€˜baseâ€™ model)
* **TTS**: `pyttsx3`

---

## ğŸ§ª Local Tips

* **Big downloads** the first time (BLIP & Whisper). Use a stable network.
* **GPU is optional**. CPU will work but be slower for BLIP/Whisper.
* If hosting on Streamlit Cloud/HF Spaces, verify FFmpeg and the `faster-whisper` wheel availability.

---

## ğŸ› Troubleshooting

* **`ModuleNotFoundError: sentence_transformers`** â†’ `pip install sentence-transformers`
* **`ImportError: cannot import name 'get_rag_response'`** â†’ Ensure folder layout matches imports in `streamlit_app.py` or fix the import path.
* **`Error in RAG pipeline: ...`** â†’ Check `GROQ_API_KEY` and confirm your FAISS files exist under `backend/vectorstore/`.
* **STT errors** â†’ Confirm FFmpeg is installed; try smaller files; upgrade `ctranslate2`.
* **BLIP cache path** on nonâ€‘Windows â†’ edit or remove `TRANSFORMERS_CACHE` line in `caption_blip.py`.

---

## ğŸ—ºï¸ Roadmap

* Inline citations (sources) in answers
* Live cloud API lookups (AWS/Azure/GCP) for realâ€‘time posture
* Better chunking + reranking (e.g., Cohere/RRF)
* Promptâ€‘guardrails & safety filters
* Dockerfile + CI for reproducible deploys

---

## ğŸ™Œ Acknowledgments

* LangChain, FAISS, HuggingFace, Salesforce BLIP, OpenAI Whisper (fasterâ€‘whisper), Streamlit, GROQ

---

## ğŸ“œ License

MIT Licensed

