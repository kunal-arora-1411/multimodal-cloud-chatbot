# üå©Ô∏è Multimodal Cloud RAG Chatbot

**Ask anything about AWS, Azure, or GCP using Text ‚Ä¢ Voice ‚Ä¢ Image.**

> Canva‚Äëready README you can paste into your repo or publish as a one‚Äëpager.

---

## ‚ú® Highlights

* **RAG over your cloud knowledge** using FAISS + Sentence‚ÄëTransformers
* **GROQ Llama‚Äë3.3‚Äë70B** for high‚Äëquality answers (via REST)
* **Modes:** Text chat ¬∑ Voice input (Whisper) ¬∑ Image caption ‚Üí query (BLIP)
* **Streamlit UI** with a simple chat history

---

## üèóÔ∏è Architecture (at a glance)

```
[User] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Streamlit UI
   ‚îÇ                          ‚îÇ
   ‚îÇ  text / voice / image    ‚îÇ
   ‚ñº                          ‚ñº
Voice (Whisper)         Image (BLIP caption)
   ‚îÇ                          ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Query text ‚óÑ‚îÄ‚îÄ‚îò
                           ‚îÇ
                     RAG Pipeline
                (FAISS + embeddings)
                           ‚îÇ
                     GROQ LLM (API)
                           ‚îÇ
                           ‚ñº
                      Final Answer
```

---

## üìÅ Project Structure (expected)

```
multimodal-cloud-chatbot/
‚îú‚îÄ backend/
‚îÇ  ‚îú‚îÄ rag_pipeline.py           # FAISS retriever + GROQ client
‚îÇ  ‚îî‚îÄ vectorstore/
‚îÇ     ‚îú‚îÄ index.faiss
‚îÇ     ‚îî‚îÄ index.pkl              # place your built index here
‚îÇ  ‚îî‚îÄ multimodal/
‚îÇ     ‚îî‚îÄ caption_blip.py        # BLIP image captioning
‚îú‚îÄ voice/
‚îÇ  ‚îú‚îÄ speech_to_text.py         # faster-whisper STT
‚îÇ  ‚îî‚îÄ text_to_speech.py         # pyttsx3 TTS
‚îú‚îÄ streamlit_app.py             # UI (Text/Voice/Image)
‚îú‚îÄ requirements.txt             # runtime deps
‚îî‚îÄ .env                         # GROQ_API_KEY=...
```

> If your repo uses a flat layout, either move files into these folders or adjust imports in `streamlit_app.py` accordingly.

---

## ‚öôÔ∏è Prerequisites

* **Python 3.10+** (3.11 OK)
* **FFmpeg** (required for audio handling; e.g., `choco install ffmpeg` on Windows, `sudo apt-get install ffmpeg` on Ubuntu)
* **Git LFS (optional)** if you store large FAISS indexes in Git

---

## üöÄ Quickstart

1. **Clone & venv**

```bash
git clone https://github.com/kunal-arora-1411/multimodal-cloud-chatbot.git
cd multimodal-cloud-chatbot
python -m venv .venv && source .venv/bin/activate  # on Windows: .venv\Scripts\activate
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
# plus a few needed by the code
pip install requests faster-whisper sentence-transformers langchain-core langchain-community
```

> If you hit platform issues with `faster-whisper`, try `pip install --upgrade ctranslate2`.

3. **Environment variables**
   Create a `.env` file at the repo root:

```ini
GROQ_API_KEY=your_groq_key_here
```

4. **Prepare the vector store**
   Place your FAISS files under `backend/vectorstore/` as:

```
backend/vectorstore/index.faiss
backend/vectorstore/index.pkl
```

(See **Build/Update the Knowledge Base** below if you need to generate these files.)

5. **Run the app**

```bash
streamlit run streamlit_app.py
```

Open the URL shown by Streamlit.

---

## üß† Build/Update the Knowledge Base (FAISS)

Here‚Äôs a simple script you can use to index a folder of Markdown/PDF/TXT docs into FAISS.

```python
# tools/build_faiss.py
from pathlib import Path
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

docs_dir = Path("docs")  # put your content here

# Load text & PDFs (extend as needed)
loaders = []
loaders.append(DirectoryLoader(str(docs_dir), glob="**/*.txt", loader_cls=TextLoader))
loaders.append(DirectoryLoader(str(docs_dir), glob="**/*.md", loader_cls=TextLoader))
loaders.append(DirectoryLoader(str(docs_dir), glob="**/*.pdf", loader_cls=PyPDFLoader))

all_docs = []
for L in loaders:
    all_docs.extend(L.load())

# Embeddings & FAISS index
emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vs = FAISS.from_documents(all_docs, emb)

out = Path("backend/vectorstore")
out.mkdir(parents=True, exist_ok=True)
vs.save_local(str(out), index_name="index")
print("Saved FAISS index to:", out)
```

Run it:

```bash
pip install pypdf  # if indexing PDFs
python tools/build_faiss.py
```

---

## üñ•Ô∏è Using the App

### 1) Text mode

* Choose **Text** in the sidebar ‚Üí type your question.
* The app retrieves top-k chunks from FAISS and asks GROQ Llama‚Äë3.3‚Äë70B to answer with context.

### 2) Voice mode

* Choose **Voice** ‚Üí upload MP3/WAV.
* Audio is transcribed with **faster‚Äëwhisper** and used as the query.
* The answer can be played back using **pyttsx3**.

### 3) Image mode

* Choose **Image** ‚Üí upload PNG/JPG.
* A BLIP caption is generated and used as the query text.

---

## üîê Secrets & Config

* **GROQ\_API\_KEY** must be present in your `.env`.
* (Optional) **TRANSFORMERS\_CACHE** can be set for BLIP model caching. If you‚Äôre not on Windows, remove or change the path in `caption_blip.py`.

---

## üß© Key Components

* **RAG pipeline**: FAISS retriever + prompt builder + GROQ REST call
* **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`
* **LLM**: `llama-3.3-70b-versatile` via `https://api.groq.com/openai/v1/chat/completions`
* **Image captioning**: `Salesforce/blip-image-captioning-base`
* **STT**: `faster-whisper` (Whisper ‚Äòbase‚Äô model)
* **TTS**: `pyttsx3`

---

## üß™ Local Tips

* **Big downloads** the first time (BLIP & Whisper). Use a stable network.
* **GPU is optional**. CPU will work but be slower for BLIP/Whisper.
* If hosting on Streamlit Cloud/HF Spaces, verify FFmpeg and the `faster-whisper` wheel availability.

---

## üêõ Troubleshooting

* **`ModuleNotFoundError: sentence_transformers`** ‚Üí `pip install sentence-transformers`
* **`ImportError: cannot import name 'get_rag_response'`** ‚Üí Ensure folder layout matches imports in `streamlit_app.py` or fix the import path.
* **`Error in RAG pipeline: ...`** ‚Üí Check `GROQ_API_KEY` and confirm your FAISS files exist under `backend/vectorstore/`.
* **STT errors** ‚Üí Confirm FFmpeg is installed; try smaller files; upgrade `ctranslate2`.
* **BLIP cache path** on non‚ÄëWindows ‚Üí edit or remove `TRANSFORMERS_CACHE` line in `caption_blip.py`.

---

## üó∫Ô∏è Roadmap

* Inline citations (sources) in answers
* Live cloud API lookups (AWS/Azure/GCP) for real‚Äëtime posture
* Better chunking + reranking (e.g., Cohere/RRF)
* Prompt‚Äëguardrails & safety filters
* Dockerfile + CI for reproducible deploys

---

## üôå Acknowledgments

* LangChain, FAISS, HuggingFace, Salesforce BLIP, OpenAI Whisper (faster‚Äëwhisper), Streamlit, GROQ

---

## üìú License

MIT (or add your preferred license)

---

## üì∑ Optional: Add a Banner/Screenshots (for Canva)

* Hero banner with title + emoji
* 3 tiles showing **Text**, **Voice**, **Image** modes
* Simple flow diagram (from the ASCII above)

